import matplotlib.pyplot as plt
import numpy as np
from numpy import ones, zeros, linspace, diag, eye, abs, mean, log, sqrt, tile, meshgrid, r_, diagflat, reshape, sign, \
    where, array, repeat, newaxis
import numpy as np
from numpy import sum as npsum, max as npmax, min as npmin
from numpy.core.umath import minimum
from numpy.linalg import eig, norm, solve, pinv

from scipy.signal import lfilter as filter

plt.style.use('seaborn')


def FitMultivariateGarch(returns, demean=1, eps=0, df=500):
    ## Estimation of multivariate GARCH models
    #  INPUTS
    #   returns : [matrix] (n_ x t_end) returns so rows must correspond to time and columns to assets
    #   demean  : [scalar] specifies whether returns should be demeaned (if demean = 1) or not to estimate the model default value is 1
    #   eps     : [scalar] used in enforcing a_ii + b_ii <= 1 - eps the default value is zero
    #   df      : [scalar] degree of freedom for the t-distribution the default value is 500 to make it, basically, normal
    #  OPS
    #   mu      : [vector]
    #   ATMF    : [matrix] coefficient matrix A-tilde (in the notation of the paper)
    #   BTMF    : [matrix] coefficient matrix B-tilde (in the notation of the paper)
    #   CTMF    : [matrix] coefficient matrix C-tilde (in the notation of the paper)
    #   Hhat    : [matrix] forecasted conditional covariance matrix
    #  NOTE
    #   Initially written by Olivier Ledoit and Michael Wolf

    if eps < 0:
        raise ValueError('eps must be a (small) positive number')

    # Initialization
    [n_, t_] = returns.shape
    if 1 == demean:
        mu = mean(returns, 1, keepdims=True)
        returns = returns - tile(mu, (1, t_))
    S = returns @ returns.T / (t_ - 1)
    x = returns

    A = zeros((n_, n_))
    B = zeros((n_, n_))
    C = zeros((n_, n_))

    # Rescale Data
    scale = sqrt(mean(x ** 2, 1, keepdims=True))
    x = x / tile(scale, (1, t_))

    # Estimation of On-Diagonal Elements
    h = zeros((n_, t_))
    for i in range(n_):
        # Likelihood Maximization
        q0, q1, q2 = garch1f4(x[i].T, eps, df)[0]
        A[i, i] = q1
        B[i, i] = q2
        C[i, i] = q0
        h[i, :] = \
        filter([0, q1], [1, -q2], x[i, :] ** 2 * (df - 2) / df, zi=array([mean(x[i, :] ** 2) * (df - 2) / df]))[0] \
        + filter([0, q0], [1, -q2], ones(t_))

    # First-step Estimation of Off-Diagonal Elements
    for i in range(n_):
        for j in range(i + 1, n_):
            # Likelihood Maximization
            theta = garch2f8(x[i, :] * x[j, :], C[i, i], A[i, i], B[i, i], x[i, :] ** 2, h[i, :], C[j, j], A[j, j],
                             B[j, j], x[j, :] ** 2, h[j, :], df)
            A[i, j] = theta[1]
            B[i, j] = theta[2]
            C[i, j] = theta[0]
            A[j, i] = A[i, j]
            B[j, i] = B[i, j]
            C[j, i] = C[i, j]

    # Transformation of Coefficient Matrices
    ATMF = minfro(A)
    BTMF = minfro(B)
    CTMF = minfro(C / (1 - B)) * (1 - BTMF)

    # Rescale
    # C = C * (scale*scale.T)
    CTMF = CTMF * (scale * scale.T)

    # Forecast of Conditional Covariance Matrix
    Hhat = zeros((n_, n_))
    for i in range(n_):
        for j in range(n_):
            hSeries = filter([0, ATMF[i, j]], [1, -BTMF[i, j]], returns[i, :].T * returns[j, :].T, zi=array([S[i, j]]))[
                          0] + \
                      filter([0, CTMF[i, j]], [1, -BTMF[i, j]], ones(t_))
            Hhat[i, j] = hSeries[t_ - 1]

    return mu, ATMF, BTMF, CTMF, Hhat


def garch1f4(x, eps, df):
    ## Fit a GARCH(1,1) model with student-t errors
    #  INPUTS
    #   x     : [vector] (T x 1) data generated by a GARCH(1,1) process
    #  OPS
    #   q     : [vector] (4 x 1) parameters of the GARCH(1,1) process
    #   qerr  : [vector] (4 x 1) standard error of parameter estimates
    #   hf    : [scalar] current conditional heteroskedasticity estimate
    #   hferr : [scalar] standard error on hf
    #  NOTE
    #   o Uses a conditional t-distribution with fixed degrees of freedom
    #   o Originally written by Olivier Ledoit, 4/28/1997
    #   o Difference with garch1f: errors come from the score alone

    # Parameters
    gold = (1 + sqrt(5)) / 2  # step size increment
    tol1 = 1e-7  # for termination criterion
    tol2 = 1e-7  # for closeness to boundary
    big = 2  # for making the hessian negative definite
    maxiter = 50  # maximum number of iterations
    n = 30  # number of points on the grid

    # Rescale
    y = (x.flatten() - mean(x.flatten())) ** 2
    t = len(y)
    scale = sqrt(mean(y ** 2))
    y = y / scale
    s = mean(y)
    # Grid search

    [ag, bg] = meshgrid(linspace(0, 1 - eps, n), linspace(0, 1 - eps, n))
    cg = np.maximum(s * (1 - ag - bg), 0)
    likeg = -np.Inf * ones((n, n))
    for i in range(n):
        for j in range(n - i):
            h = filter(array([0, ag[i, j]]), array([1, -bg[i, j]]), y * (df - 2) / df, zi=array([s * (df - 2) / df]))[0] \
                + filter(array([0, cg[i, j]]), array([1, -bg[i, j]]), ones(t))
            likeg[i, j] = -npsum(log(h) + (df + 1) * log(1 + y / h / df))

    maxlikeg = npmax(likeg)
    maxima = where(likeg == maxlikeg)  ##ok<MXFND>

    # Initialize optimization
    a = r_[cg[maxima], ag[maxima], bg[maxima]]
    best = 0
    da = 0
    # term   = 1
    # negdef = 0
    iter = 0

    # Begin optimization loop
    while iter < maxiter:
        iter = iter + 1

        # New parameter1
        a = a + gold ** best * da

        # Conditional variance
        h = filter([0, a[1]], [1, -a[2]], y * (df - 2) / df, zi=array([s * (df - 2) / df]))[0] \
            + filter([0, a[0]], [1, -a[2]], ones(t))

        # Likelihood
        if (any(a < 0) or ((a[1] + a[2]) > 1 - eps)):
            like = -np.Inf
        else:
            like = -npsum(log(h) + (df + 1) * log(1 + y / h / df))

        # Gradient
        GG = r_['-1', filter([0, 1], [1, -a[2]], ones(t))[..., newaxis],
                filter([0, 1], [1, -a[2]], y * (df - 2) / df)[..., newaxis],
                filter([0, 1], [1, -a[2]], h)[..., newaxis]]
        g1 = ((df + 1) * (y / (y + df * h)) - 1) / h
        G = GG * repeat(g1.reshape(-1, 1), 3, axis=1)
        gra = npsum(G, axis=0)

        # Hessian
        GG2 = GG[:, [0, 1, 2, 0, 1, 2, 0, 1, 2]] * GG[:, [0, 0, 0, 1, 1, 1, 2, 2, 2]]
        g2 = -((df + 1) * (y / (y + df * h)) - 1) / h ** 2 - (df * (df + 1)) * (y / (y + df * h) ** 2 / h)
        HH = zeros((t, 9))
        HH[:, 2] = filter([0, 1], [1, -a[2]], GG[:, 0])
        HH[:, 6] = HH[:, 2]
        HH[:, 5] = filter([0, 1], [1, -a[2]], GG[:, 1])
        HH[:, 7] = HH[:, 5]
        HH[:, 8] = filter([0, 2], [1, -a[2]], GG[:, 2])
        H = GG2 * repeat(g2.reshape(-1, 1), 9, axis=1) + HH * repeat(g1.reshape(-1, 1), 9, axis=1)
        hes = reshape(npsum(H, axis=0), (3, 3), 'F')

        # Negative definite
        d, u = eig(hes)
        # d = diagflat(d)
        if any(d > 0):
            negdef = 0
            d = min(d, max(d[d < 0]) / big)
            hes = u @ diagflat(d) @ u.T
        else:
            negdef = 1

        # Direction
        da = -gra.dot(pinv(hes))

        # Termination criterion
        term = da @ gra.T
        if (term < tol1) and negdef:
            break

        # Step search
        best = 0
        newa = a + gold ** (best - 1) * da
        if (any(newa < 0) or (newa[1] + newa[2] > 1 - eps)):
            left = -np.Inf
        else:
            h = filter([0, newa[1]], [1, -newa[2]], y * (df - 2) / df, zi=array([s * (df - 2) / df]))[0] \
                + filter([0, newa[0]], [1, -newa[2]], ones(t))
            left = -sum(log(h) + (df + 1) * log(1 + y / h / df))

        newa = a + gold ** best * da
        if (any(newa < 0) or (newa[1] + newa[2] > 1 - eps)):
            center = -np.Inf
        else:
            h = filter([0, newa[1]], [1, -newa[2]], y * (df - 2) / df, zi=array([s * (df - 2) / df]))[0] \
                + filter([0, newa[0]], [1, -newa[2]], ones(t))
            center = -sum(log(h) + (df + 1) * log(1 + y / h / df))

        newa = a + gold ** (best + 1) * da
        if (any(newa < 0) or (newa[1] + newa[2] > 1 - eps)):
            right = -np.Inf
        else:
            h = filter([0, newa[1]], [1, -newa[2]], y * (df - 2) / df, zi=array([s * (df - 2) / df]))[0] \
                + filter([0, newa[0]], [1, -newa[2]], ones(t))
            right = -sum(log(h) + (df + 1) * log(1 + y / h / df))

        if all(like > array([left, center, right])) or all(left > array([center, right])):
            while True:
                best = best - 1
                center = left
                newa = a + gold ** (best - 1) * da
                if (any(newa < 0) or (newa[1] + newa[2] > 1 - eps)):
                    left = -np.Inf
                else:
                    h = filter([0, newa[1]], [1, -newa[2]], y * (df - 2) / df, zi=array([s * (df - 2) / df]))[0] \
                        + filter([0, newa[0]], [1, -newa[2]], ones(t))
                    left = -sum(log(h) + (df + 1) * log(1 + y / h / df))

                if all(center >= array([like, left])):
                    break


        elif all(right > array([left, center])):
            while True:
                best = best + 1
                center = right
                newa = a + gold ** (best + 1) * da
                if (any(newa < 0) or (newa[1] + newa[2]) > 1 - eps):
                    right = -np.Inf
                else:
                    h = filter([0, newa[1]], [1, -newa[2]], y * (df - 2) / df, zi=array([s * (df - 2) / df]))[0] \
                        + filter([0, newa[0]], [1, -newa[2]], ones(t))
                    right = -npsum(log(h) + (df + 1) * log(1 + y / h / df))

                if center > right:
                    break

        # If stuck at boundary then stop
        if (center == like) and (any(a < tol2) or (a[1] + a[2]) > 1 - tol2):
            break

        # End of optimization loop

    a[a < tol2] = zeros(len(a[a < tol2]))
    if a[1] + a[2] > 1 - tol2:
        if a[1] < 1 - tol2:
            a[1] = a[1] + (1 - a[1] - a[2])
        else:
            a[2] = a[2] + (1 - a[1] - a[2])

    # Estimation error and volatility forecast
    # aerr=inv(G.T@G)
    tmp = (G.T @ G)
    aerr = tmp.dot(pinv(eye(tmp.shape[0])))
    hf = a[0] + a[1] * y[t - 1] * (df - 2) / df + a[2] * h[t - 1]
    gf = r_[1, y[t - 1], h[t - 1]] + a[2] * GG[t - 1, :]
    hferr = gf @ aerr @ gf.T
    aerr = diagflat(aerr).T

    # Revert to original scale
    a[0] = a[0] * scale
    aerr[0] = aerr[0] * scale ** 2
    hf = hf * scale
    hferr = hferr * scale ** 2

    aerr = sqrt(aerr)
    hferr = sqrt(hferr)
    q = a
    qerr = aerr

    return q, qerr, hf, hferr


def garch2f8(y, c1, a1, b1, y1, h1, c2, a2, b2, y2, h2, df):
    ## Off-diagonal parameter estimation in bivariate GARCH(1,1) when diagonal parameters are given.
    #  INPUTS
    #   y     : [vector] (T x 1) data generated by a GARCH(1,1) process
    #  OPS
    #   q     : [vector] (4 x 1) parameters of the GARCH(1,1) process
    #   qerr  : [vector] (4 x 1) standard error of parameter estimates
    #   hf    : [scalar] current conditional heteroskedasticity estimate
    #   hferr : [scalar] standard error on hf
    #  NOTE
    #   o Originally written by Olivier Ledoit, 4/28/1997
    #   o Uses a conditional t-distribution with fixed degrees of freedom
    #   o Steepest Ascent on boundary, Hessian off boundary, no grid search

    # Parameters
    gold = (1 + sqrt(5)) / 2  # step size increment
    tol1 = 1e-7  # for termination criterion
    tol2 = 1e-7  # for closeness to boundary
    big = 2  # for making the hessian negative definite
    maxiter = 50  # maximum number of iterations
    # n=30			# number of points on the grid

    # Prepare
    t = len(y)
    y1 = y1.flatten()
    y2 = y2.flatten()
    y = y.flatten()
    s = mean(y)
    # s1=mean((y1))
    # s2=mean((y2))
    h1 = h1.flatten()
    h2 = h2.flatten()

    # Bounds
    low = r_[-sqrt(c1 * c2), 0, 0] + tol2
    high = r_[sqrt(c1 * c2), sqrt(a1 * a2), sqrt(b1 * b2)] - tol2

    # Starting Point
    a0 = 0.9 * sqrt(a1 * a2)
    b0 = 0.9 * sqrt(b1 * b2)
    c0 = mean(y) * (1 - a0 - b0) * (df - 2) / df
    c0 = sign(c0) * min(abs(c0), 0.9 * sqrt(c1 * c2))

    # Initialize optimization
    a = r_[c0, a0, b0]
    best = 0
    da = 0
    # term=1
    # negdef=0
    iter = 0

    # Begin optimization loop
    while iter < maxiter:
        iter = iter + 1

        # New parameter
        # olda = a
        a = a + gold ** best * da

        # Conditional variance
        h = filter([0, a[1]], [1, -a[2]], y * (df - 2) / df, zi=array([s * (df - 2) / df]))[0] \
            + filter([0, a[0]], [1, -a[2]], ones(t))
        d = h1 * h2 - h ** 2
        z = h2 * y1 + h1 * y2 - 2 * h * y

        # Likelihood
        if (any(a < low) or any(a > high)):
            like = -np.Inf
        else:
            # like=-sum(log(h)+y/h))
            # like=-sum(log(h)+(df+1)*log(1+y/h/df))
            if any(d <= 0) or any(1 + z / d / df <= 0):
                like = -np.Inf
            else:
                like = -sum(log(d) + (2 + df) * log(1 + z / d / df)) / 2

        # Gradient
        GG = r_['-1', filter([0, 1], [1, -a[2]], ones(t))[..., newaxis],
                filter([0, 1], [1, -a[2]], y * (df - 2) / df)[..., newaxis],
                filter([0, 1], [1, -a[2]], h)[..., newaxis]]
        g1 = h / d + (2 + df) * y / (z + d * df) - (2 + df) * h * z / (z + d * df) / d
        G = GG * repeat(g1.reshape(-1, 1), 3, axis=1)
        gra = npsum(G, axis=0)

        # Hessian
        GG2 = GG[:, [0, 1, 2, 0, 1, 2, 0, 1, 2]] * GG[:, [0, 0, 0, 1, 1, 1, 2, 2, 2]]
        g2 = 1 / d + 2 * h ** 2 / d ** 2 - (2 + df) * y / (z + d * df) ** 2 * (-2 * y - 2 * df * h) \
             - (2 + df) * z / (z + d * df) / d + 2 * (2 + df) * h * y / (z + d * df) / d \
             + (2 + df) * h * z / (z + d * df) ** 2 / d * (-2 * y - 2 * df * h) \
             - 2 * (2 + df) * h ** 2 * z / (z + d * df) / d ** 2
        HH = zeros((t, 9))
        HH[:, 2] = filter([0, 1], [1, -a[2]], GG[:, 0])
        HH[:, 6] = HH[:, 2]
        HH[:, 5] = filter([0, 1], [1, -a[2]], GG[:, 1])
        HH[:, 7] = HH[:, 5]
        HH[:, 8] = filter([0, 2], [1, -a[2]], GG[:, 2])
        H = GG2 * repeat(g2.reshape(-1, 1), 9, axis=1) + HH * repeat(g1.reshape(-1, 1), 9, axis=1)
        hes = reshape(npsum(H, axis=0), (3, 3), 'F')

        # Negative definite
        val, u = eig(hes)
        if all(val > 0):
            hes = -eye(3)
            negdef = 0
        elif any(val > 0):
            negdef = 0
            val = minimum(val, max(val[val < 0]) / big)
            hes = u @ diagflat(val) @ u.T
        else:
            negdef = 1

        # Steepest Ascent or Newton
        if any(a == low) or any(a == high):
            da = -((gra @ gra.T) / (gra @ hes @ gra.T)) * gra
        else:
            da = -gra.dot(pinv(hes))

        # Termination criterion
        term = da @ gra.T
        if ((term < tol1) and negdef):
            break

            # If you are on the boundary and want to get out, slide along
        da[(a == low) & (da < 0)] = zeros(da[(a == low) & (da < 0)].shape)
        da[(a == high) & (da > 0)] = zeros(da[(a == high) & (da > 0)].shape)

        # If you are stuck in a corner, terminate too
        if all(da == 0):
            break

        # Go no further than next boundary
        hit = r_[(low[da != 0] - a[da != 0]) / da[da != 0],
                 (high[da != 0] - a[da != 0]) / da[da != 0]]
        hit = hit[hit > 0]
        da = min(r_[hit, 1]) * da

        # Step search
        best = 0
        newa = a + gold ** (best - 1) * da
        if (any(newa < low) or any(newa > high)):
            left = -np.Inf
        else:
            h = filter([0, newa[1]], [1, -newa[2]], y * (df - 2) / df, zi=array([s * (df - 2) / df]))[0] \
                + filter([0, newa[0]], [1, -newa[2]], ones(t))
            d = h1 * h2 - h ** 2
            z = h2 * y1 + h1 * y2 - 2 * h * y
            if any(d <= 0) or any(1 + z / d / df <= 0):
                left = -np.Inf
            else:
                left = -sum(log(d) + (2 + df) * log(1 + z / d / df)) / 2

        newa = a + gold ** best * da
        if (any(newa < low) or any(newa > high)):
            center = -np.Inf
        else:
            h = filter([0, newa[1]], [1, -newa[2]], y * (df - 2) / df, zi=array([s * (df - 2) / df]))[0] \
                + filter([0, newa[0]], [1, -newa[2]], ones(t))
            d = h1 * h2 - h ** 2
            z = h2 * y1 + h1 * y2 - 2 * h * y
            if any(d <= 0) or any(1 + z / d / df <= 0):
                center = -np.Inf
            else:
                center = -sum(log(d) + (2 + df) * log(1 + z / d / df)) / 2

        newa = a + gold ** (best + 1) * da
        if (any(newa < low) or any(newa > high)):
            right = -np.Inf
        else:
            h = filter([0, newa[1]], [1, -newa[2]], y * (df - 2) / df, zi=array([s * (df - 2) / df]))[0] \
                + filter([0, newa[0]], [1, -newa[2]], ones(t))
            d = h1 * h2 - h ** 2
            z = h2 * y1 + h1 * y2 - 2 * h * y
            if any(d <= 0) or any(1 + z / d / df <= 0):
                right = -np.Inf
            else:
                right = -sum(log(d) + (2 + df) * log(1 + z / d / df)) / 2

        if all(like > array([left, center, right])) or all(left > array([center, right])):
            while True:
                best = best - 1
                center = left
                newa = a + gold ** (best - 1) * da
                if (any(newa < low) or any(newa > high)):
                    left = -np.Inf
                else:
                    h = filter([0, newa[1]], [1, -newa[2]], y * (df - 2) / df, zi=array([s * (df - 2) / df]))[0] \
                        + filter([0, newa[0]], [1, -newa[2]], ones(t))
                    d = h1 * h2 - h ** 2
                    z = h2 * y1 + h1 * y2 - 2 * h * y
                    if any(d <= 0) or any(1 + z / d / df <= 0):
                        left = -np.Inf
                    else:
                        left = -sum(log(d) + (2 + df) * log(1 + z / d / df)) / 2
                if all(center >= [like, left]):
                    break


        elif all(right > array([left, center])):
            while True:
                best = best + 1
                center = right
                newa = a + gold ** (best + 1) * da
                if (any(newa < low) or any(newa > high)):
                    right = -np.Inf
                else:
                    h = filter([0, newa[1]], [1, -newa[2]], y * (df - 2) / df, zi=array([s * (df - 2) / df]))[0] \
                        + filter([0, newa[0]], [1, -newa[2]], ones(t))
                    d = h1 * h2 - h ** 2
                    z = h2 * y1 + h1 * y2 - 2 * h * y
                    if any(d <= 0) or any(1 + z / d / df <= 0):
                        right = -np.Inf
                    else:
                        right = -npsum(log(d) + (2 + df) * log(1 + z / d / df)) / 2
                if center > right:
                    break
    q = a

    return q


def minfro(A):
    #  INPUTS
    #   A   : [matrix] an indefinite symmetric matrix with non-negative diagonal elements
    #  OPS
    #   XXX : [matrix] positive semi-definite matrix with same diagonal elements as A that is closest
    #         to A according to the Frobenius norm
    #  NOTE
    #   o Written initially by Ilya Sharapov (1997)

    if any(diag(A) < 0):
        raise ValueError('Diagonal Elements Must Be Non-Negative!')
    elif npsum(A != A.T) != 0:
        raise ValueError('Matrix Must Be Symmetric!')
    elif all(eig(A)[0] >= 0):
        XXX = A
    else:
        # if things go wrong make rho bigger and wait longer
        rho = 0.75
        tol = 3e-6  # tolerance
        maxj = 10  # max number of iterations

        n = A.shape[0]
        # [n, nn] = A.shape
        M = diagflat(diag(A))  # initialize with diagonal
        # [n, nn] = A.shape
        oldnorm = norm(M - A, ord='fro')
        oldnormj = oldnorm
        normj[0] = oldnorm

        j = 1
        incmax = 1e32  # just to enter the loop
        while ((j < maxj) and (incmax > tol)):
            incmax = 0
            for i in range(n):
                a = r_[A[:i, i], A[i + 1:n, i]]
                m = r_[M[:i, i], M[i + 1:n, i]]
                aii = A(i, i)
                b = a - rho @ m
                # Newton's step
                x = newton(M, i, b, m, aii, n, rho)
                P = eye(n)
                P[i, :n] = x.T  ##ok<SPRIX>
                # update
                Mtest = P @ M @ P.T
                M = Mtest
                inc = oldnorm - norm(M - A, ord='fro')
                oldnorm = norm(M - A, ord='fro')
                # find maximal increment over iteration
                if inc > incmax:
                    incmax = inc

            normj[j + 1] = oldnorm  ##ok<AGROW>
            # incj[j]    = oldnormj-oldnorm
            # oldnormj   = oldnorm
            j = j + 1

        XXX = M

    return XXX


def newton(M, i, b, m, aii, n, rho):
    ## Newton setp
    # Subroutine called interbally by minfro.m

    maxit = 40
    eps = 1e-9  # small correction

    # to handle singularity
    l = 0.0
    MM = r_['-1', M[:i, :i], M[:i, i:n], M[i:n, :i], M[i:n, i:n]] + eps @ eye(n - 1)

    j = 1
    # loop
    while j < maxit:
        tmp = MM @ MM + l @ MM
        IM = solve(tmp, eye(tmp))
        # IM = inv(MM@MM+l@MM)
        x = IM @ (MM @ b - l @ rho @ m)
        f = rho @ rho @ aii + 2 * rho @ x.T @ m + x.T @ MM @ x - aii

        if abs(f) < 1e-7:
            break

        dfdl = -2 * (rho @ m + MM @ x).T @ IM @ (rho @ m + MM @ x)
        # Newton's step
        l = l - f / dfdl
        j = j + 1

    if abs(f) < 1e-7:
        # converged
        xx = r_[x[:i - 1], rho, x[i:n]]
    else:
        # didn't converge
        xx = zeros((n, 1))
        xx[i] = 1
    return xx
